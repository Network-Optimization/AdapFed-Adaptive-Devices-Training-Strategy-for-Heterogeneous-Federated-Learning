{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/test/zhouying/完整交付代码')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from utils.plots import ExperimentLogger\n",
    "from method.clustered.clustered import clustered_fl\n",
    "from method.my.my import my_fl\n",
    "from method.fedavg.fedavg import fed_avg_fl\n",
    "from method.ditto.ditto import ditto_fl\n",
    "from method.local.local import local_fl\n",
    "from init_devices import init_clients_and_server\n",
    "from init_datasets import load_dataset\n",
    "import argparse\n",
    "from utils.plots import draw_result_table\n",
    "torch.cuda.set_device(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    \"\"\"parse the command line args\n",
    "\n",
    "    Returns:\n",
    "        args: a namespace object including args\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=__doc__)\n",
    "    parser.add_argument(\n",
    "        '--dataset',\n",
    "        help=\"name of dataset;\"\n",
    "        \" possible are `EMNIST`, `FashionMNIST`, `CIFAR10`，`CIFAR100`, `Shakespeare`\",\n",
    "        type=str,\n",
    "        default='CIFAR10'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--method',\n",
    "        help = \"the method to be used;\"\n",
    "               \" possible are `My`,`Clustered`, `FedAvg`, `Ditto`, `Local`, 'Overlap'\",\n",
    "        type=str,\n",
    "        default='My'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--pathological_split',\n",
    "        help='if selected, the dataset will be split as in'\n",
    "             '\"Communication-Efficient Learning of Deep Networks from Decentralized Data\";'\n",
    "             'i.e., each client will receive `n_shards` of dataset, where each shard contains at most two classes',\n",
    "        action='store_true'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--n_shards',\n",
    "        help='number of shards given to each clients/task; ignored if `--pathological_split` is not used;'\n",
    "             'default is 2',\n",
    "        type=int,\n",
    "        default=2\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--n_clients',\n",
    "        help = \"the number of clients\",\n",
    "        type=int,\n",
    "        default=10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--n_clusters',\n",
    "        help = \"initialize the number of cluster of data distribution\",\n",
    "        type=int,\n",
    "        default=3\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--alpha',\n",
    "        help = \"the parameter of dirichlet\",\n",
    "        type=float,\n",
    "        default=1.0\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--local_epochs',\n",
    "        help='number of local epochs before communication; default is 1',\n",
    "        type=int,\n",
    "        default=1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_rounds\",\n",
    "        help=\"number of communication rounds\",\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_sample\",\n",
    "        help=\"number of sample to use\",\n",
    "        type=int,\n",
    "        default=20000\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_frac\",\n",
    "        help=\"fraction of train samples\",\n",
    "        type=float,\n",
    "        default=0.8\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--val_frac\",\n",
    "        help=\"fraction of validation samples in train samples\",\n",
    "        type=float,\n",
    "        default=0\n",
    "    )   \n",
    "    parser.add_argument(\n",
    "        \"--seed\",\n",
    "        help='random seed',\n",
    "        type=int,\n",
    "        default=42\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动创建 Namespace 对象并设置属性\n",
    "args = argparse.Namespace(\n",
    "    dataset='CIFAR10',\n",
    "    method='PFCAL',\n",
    "    pathological_split=False,\n",
    "    n_shards=2,\n",
    "    n_clients=20,\n",
    "    n_clusters=3,\n",
    "    alpha=1.0,\n",
    "    local_epochs=1,\n",
    "    n_rounds=100,\n",
    "    n_sample=40000,\n",
    "    train_frac=0.8,\n",
    "    val_frac=0.0,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# 将 Namespace 对象转换为字典\n",
    "args_dict = vars(args)\n",
    "print(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def Com_clinet(clients,similarities):\n",
    "    a=deepcopy(clients)\n",
    "    similarity_matrix = np.array(similarities)  # 确保相似性矩阵已经正确定义和初始化\n",
    "    # 尝试不同的聚类数目\n",
    "    best_num_clusters = 2  # 假设2为起始聚类数目\n",
    "    best_silhouette = -np.inf\n",
    "\n",
    "    for n_clusters in range(2, 10):  # 尝试从2到9个聚类\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "        kmeans.fit(similarity_matrix)\n",
    "        \n",
    "        # 计算轮廓系数\n",
    "        silhouette_avg = silhouette_score(similarity_matrix, kmeans.labels_)\n",
    "        \n",
    "        # 记录最优的聚类数目\n",
    "        if silhouette_avg > best_silhouette:\n",
    "            best_silhouette = silhouette_avg\n",
    "            best_num_clusters = n_clusters\n",
    "            best_labels = kmeans.labels_  # 保存当前最优的聚类标签\n",
    "        \n",
    "        # print(f\"最优的聚类数目是: {best_num_clusters}\")\n",
    "        data_points = list(range(len(similarity_matrix)))\n",
    "        clustered_data_points = {}\n",
    "        for user, label in zip(data_points, best_labels):\n",
    "            clustered_data_points.setdefault(label, []).append(user)\n",
    "        res_idc=[]\n",
    "        for cluster_id, users in clustered_data_points.items():\n",
    "            res_idc.append(users)\n",
    "        print(res_idc)\n",
    "        return res_idc\n",
    "    \n",
    "    print(res_idc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(source):\n",
    "    return torch.cat([value.flatten() for value in source.values()])\n",
    "def pairwise_angles(sources):\n",
    "    angles = torch.zeros([len(sources), len(sources)])\n",
    "    for i, source1 in enumerate(sources):\n",
    "        for j, source2 in enumerate(sources):\n",
    "            s1 = flatten(source1)\n",
    "            s2 = flatten(source2)\n",
    "            angles[i,j] = torch.sum(s1*s2)/(torch.norm(s1)*torch.norm(s2)+1e-12)\n",
    "    return angles.numpy()\n",
    "\n",
    "def compute_pairwise_similarities(clients):\n",
    "    return pairwise_angles([client.dW for client in clients])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "dataset, client_train_idcs, client_test_idcs, client_val_idcs, data_info = load_dataset(args)\n",
    "# print(dataset)\n",
    "# print(client_train_idcs)\n",
    "# print(len(client_train_idcs))\n",
    "# print(len(client_test_idcs))\n",
    "# print(data_info)\n",
    "# #     # print(dataset[0])\n",
    "# #     # print(client_train_idcs)\n",
    "# #     # print(client_test_idcs)\n",
    "clients, server = init_clients_and_server(args, dataset, client_train_idcs, client_test_idcs, client_val_idcs, data_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import svd\n",
    "\n",
    "print(len(client_train_idcs))\n",
    "# print(np.vstack([torch.flatten(dataset[i][0]) for i in client_train_idcs[0]]))\n",
    "principal_component_list=[]\n",
    "for idx in range(args.n_clients):\n",
    "    # print(np.vstack([torch.flatten(dataset[i][0]) for i in client_train_idcs[0]]))\n",
    "    matrix=np.vstack([torch.flatten(dataset[i][0]) for i in client_train_idcs[idx]])\n",
    "    U, S, Vt = svd(matrix)\n",
    "    principal_component = U[:, 0]\n",
    "    # print(principal_component)\n",
    "    principal_component_list.append(principal_component)\n",
    "    # break\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "n = len(principal_component_list)\n",
    "similarity_matrix = np.zeros((n, n))\n",
    "\n",
    "# 计算第一个主成分向量之间的相似度\n",
    "for i in range(len(principal_component_list)):\n",
    "    for j in range(i + 1, len(principal_component_list)):\n",
    "        sim = cosine_similarity(principal_component_list[i][0:min(len(principal_component_list[i]),len(principal_component_list[j]))], principal_component_list[j][0:min(len(principal_component_list[i]),len(principal_component_list[j]))])\n",
    "        # print(f\"主成分向量 {i} 和 {j} 的相似度: {sim:.4f}\")\n",
    "        similarity_matrix[i, j] = sim\n",
    "        similarity_matrix[j, i] = sim\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 将相似度矩阵转换为距离矩阵 (1 - 相似度)\n",
    "distance_matrix = 1 - similarity_matrix\n",
    "\n",
    "# 使用层次聚类 (基于距离矩阵)\n",
    "Z = linkage(distance_matrix, method='average')\n",
    "\n",
    "# 选择聚类数量，例如3个簇\n",
    "n_clusters = 3\n",
    "clusters = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "\n",
    "# 计算每个簇的质心并找到最接近质心的点\n",
    "def find_closest_to_centroid_from_similarity(similarity_matrix, labels, n_clusters, num_points=1):\n",
    "    closest_indices = []  # 存储最接近质心的点的索引\n",
    "    closest_indices_list=[]\n",
    "    for cluster_id in range(1, n_clusters + 1):\n",
    "        # 找到属于该簇的向量索引\n",
    "        cluster_indices = np.where(labels == cluster_id)[0]\n",
    "        cluster_sim_matrix = similarity_matrix[np.ix_(cluster_indices, cluster_indices)]\n",
    "\n",
    "        # 计算该簇的质心（通过计算相似度矩阵的均值来近似质心）\n",
    "        centroid_sim = np.mean(cluster_sim_matrix, axis=0)\n",
    "\n",
    "        # 计算每个点到质心的相似度，距离最小的点最接近质心\n",
    "        distances = 1 - centroid_sim\n",
    "\n",
    "        # 找到距离最小的 num_points 个点，返回它们的原始索引\n",
    "        closest_idx_in_cluster = np.argsort(distances)[:num_points]\n",
    "        closest_indices.append(list(cluster_indices[closest_idx_in_cluster]))\n",
    "        closest_indices_list.extend(list(cluster_indices[closest_idx_in_cluster]))\n",
    "\n",
    "    return closest_indices,closest_indices_list\n",
    "\n",
    "# 找到每个簇中最接近质心的点，返回它们的索引\n",
    "closest_indices,closest_indices_list = find_closest_to_centroid_from_similarity(similarity_matrix, clusters, n_clusters, num_points=5)\n",
    "\n",
    "# 输出最接近质心的点的索引\n",
    "print(clusters)\n",
    "print(\"最接近质心的点的索引：\")\n",
    "print(closest_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(closest_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plots import display_train_stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def PFCAL(args, clients, server, cfl_stats):\n",
    "\n",
    "    EPS_1 = 0.4\n",
    "    EPS_2 = 1.6   \n",
    "    cluster_indices = [[client_id for client_id in range(len(clients))]]\n",
    "    \n",
    "    acc_clients = []\n",
    "    pbar = tqdm(total=args.n_rounds)\n",
    "    client_clusters=[]\n",
    "    \n",
    "    for c_round in range(1, args.n_rounds+1):\n",
    "        # print(cluster_indices)\n",
    "        # c_round为当前的通信轮数\n",
    "        # 初始时需要保证客户端和服务器模型同步\n",
    "        if c_round == 1:\n",
    "            for client in clients:\n",
    "                client.synchronize_with_server(server)\n",
    "        # participating_clients = server.select_clients(clients, frac=1.0)\n",
    "        if c_round<50:\n",
    "            participating_clients = server.select_clients_1(clients, select_list=closest_indices_list)\n",
    "        else:\n",
    "            participating_clients = server.select_clients(clients, frac=1.0)\n",
    "\n",
    "        # for client in participating_clients:\n",
    "        #     train_stats = client.compute_weight_update(epochs=args.local_epochs)\n",
    "        #     client.reset()\n",
    "        # max_norm = server.compute_max_update_norm(clients)\n",
    "        # # 计算某个簇内的client的平均参数范数\n",
    "        # mean_norm = server.compute_mean_update_norm(clients)\n",
    "        # similarities = compute_pairwise_similarities(clients)\n",
    "        mean_norm=0\n",
    "        max_norm=0\n",
    "        # client_clusters = [[clients[i] for i in idcs] for idcs in cluster_indices]\n",
    "        # if c_round == 1:\n",
    "        #     res_idc=Com_clinet(clients,similarities)\n",
    "        #     client_clusters = [[clients[i] for i in idcs] for idcs in res_idc]\n",
    "        #     print(\"res_idc\",res_idc)\n",
    "        # if c_round==1:\n",
    "        client_clusters=[[clients[i] for i in idcs] for idcs in closest_indices]\n",
    "            # print(closest_indices)\n",
    "\n",
    "        if c_round<50:\n",
    "            # print(\"clients\",clients)\n",
    "            server.aggregate_weight_updates(participating_clients)\n",
    "        else:\n",
    "            # print(client_clusters)\n",
    "            for idx in range(len(client_clusters)):\n",
    "                server.aggregate_weight_updates(client_clusters[idx])\n",
    "    \n",
    "        for client in clients:\n",
    "            client.synchronize_with_server(server)    \n",
    "        \n",
    "        acc_clients = [client.evaluate() for client in participating_clients]\n",
    "        # print(np.mean(acc_clients))\n",
    "        cfl_stats.log({\"acc_clients\" : acc_clients, \"mean_norm\" : mean_norm, \"max_norm\" : max_norm,\"rounds\" : c_round, \"clusters\" : cluster_indices})\n",
    "        pbar.update(1)\n",
    "        display_train_stats(cfl_stats, EPS_1, EPS_2, args.n_rounds)\n",
    "    for idc in cluster_indices:    \n",
    "        server.cache_model(idc, clients[idc[0]].W, acc_clients)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.plots import display_train_stats\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "# def PFCAL(args, clients, server, cfl_stats):\n",
    "\n",
    "#     EPS_1 = 0.4\n",
    "#     EPS_2 = 1.6   \n",
    "\n",
    "#     cluster_indices = [[client_id for client_id in range(len(clients))]]\n",
    "        \n",
    "#     # similarities = server.compute_pairwise_similarities(clients)\n",
    "#     #print(similarities)\n",
    "#     # 进行n_rounds轮迭代\n",
    "#     acc_clients = []\n",
    "#     pbar = tqdm(total=args.n_rounds)\n",
    "#     client_clusters=[]\n",
    "    \n",
    "#     for c_round in range(1, args.n_rounds+1):\n",
    "#         # print(cluster_indices)\n",
    "#     # c_round为当前的通信轮数\n",
    "#         # 初始时需要保证客户端和服务器模型同步\n",
    "#         if c_round == 1:\n",
    "#             for client in clients:\n",
    "#                 client.synchronize_with_server(server)\n",
    "                \n",
    "#         participating_clients = server.select_clients(clients, frac=0.5)\n",
    "\n",
    "#         for client in participating_clients:\n",
    "#             train_stats = client.compute_weight_update(epochs=args.local_epochs)\n",
    "#             client.reset()\n",
    "#         max_norm = server.compute_max_update_norm(clients)\n",
    "#         # 计算某个簇内的client的平均参数范数\n",
    "#         mean_norm = server.compute_mean_update_norm(clients)\n",
    "#         similarities = compute_pairwise_similarities(clients)\n",
    "#         if c_round == 20:\n",
    "#             res_idc=Com_clinet(clients,similarities)\n",
    "#             client_clusters = [[clients[i] for i in idcs] for idcs in res_idc]\n",
    "#             print(client_clusters)\n",
    "            \n",
    "#         if client_clusters==[]:\n",
    "#             # print(\"clients\",clients)\n",
    "#             server.aggregate_weight_updates(clients)\n",
    "#         else:\n",
    "#             for idx in range(len(client_clusters)):\n",
    "#                 server.aggregate_weight_updates(client_clusters[idx])\n",
    "    \n",
    "#         for client in clients:\n",
    "#             client.synchronize_with_server(server)    \n",
    "        \n",
    "\n",
    "#         acc_clients = [client.evaluate() for client in clients]\n",
    "#         cfl_stats.log({\"acc_clients\" : acc_clients, \"mean_norm\" : mean_norm, \"max_norm\" : max_norm,\"rounds\" : c_round, \"clusters\" : cluster_indices})\n",
    "#         pbar.update(1)\n",
    "#         display_train_stats(cfl_stats, EPS_1, EPS_2, args.n_rounds)\n",
    "#     for idc in cluster_indices:    \n",
    "#         server.cache_model(idc, clients[idc[0]].W, acc_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(args, clients, server):\n",
    "                    \n",
    "    cfl_stats = ExperimentLogger()\n",
    "\n",
    "    if args.method == \"Clustered\":\n",
    "        clustered_fl(args, clients, server, cfl_stats)\n",
    "    elif args.method == \"My\":\n",
    "        my_fl(args, clients, server, cfl_stats)    \n",
    "    elif args.method == \"FedAvg\":\n",
    "        fed_avg_fl(args, clients, server, cfl_stats)\n",
    "    elif args.method == \"Ditto\":\n",
    "        ditto_fl(args, clients, server, cfl_stats)\n",
    "    elif args.method == \"Local\":\n",
    "        local_fl(args, clients, server, cfl_stats)\n",
    "    elif args.method == \"PFCAL\":\n",
    "        PFCAL(args, clients, server, cfl_stats)\n",
    "    else:\n",
    "        raise IOError(\"possible are `My`,`Clustered`, `FedAvg`, `Ditto`, `Local`, `Overlap`\")\n",
    "    # elif args.method == \"Overlap\":\n",
    "    #     overlap_fl(args, clients, server, cfl_stats)\n",
    "    draw_result_table(args, clients, server)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = False\n",
    "run_experiment(args, clients, server)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhouying",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
